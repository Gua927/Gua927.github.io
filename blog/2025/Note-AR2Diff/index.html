<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Training-Free Method for Parallel Decoding of Autoregressive Models | Runze Tian </title> <meta name="author" content="R. Z. Tian"> <meta name="description" content="This blog post investigates the possibility of parallel decoding for autoregressive models. The author notes that autoregressive and diffusion models both fundamentally model data probability distributions, and that each has advantages—autoregressive models in training and diffusion models in sampling. The goal is to achieve a training-free way to perform parallel decoding with a pretrained autoregressive model, enabling low-cost accelerated generation."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/moonshot.png?ac494055d0715958a4a25e163d4405f7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gua927.github.io/blog/2025/Note-AR2Diff/"> <script src="/assets/js/theme.js?8a9658bcfd3c4d23968422c4b9dc98ec"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script>
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)'],
        ],
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]'],
        ],
        processEscapes: true,
        processEnvironments: true,
        processRefs: true,
        digits: /^(?:[0-9]+(?:\{,\}[0-9]{3})*(?:\.[0-9]*)?|\.[0-9]+)/,
        maxBuffer: 5 * 1024,
      },
      chtml: {
        scale: 1,
        minScale: 0.5,
        matchFontHeight: true,
        linebreaks: { automatic: false, width: 'container' },
      },
      startup: {
        pageReady: () => {
          return MathJax.startup.defaultPageReady().then(() => {
            console.log('MathJax typesetting complete');
          });
        },
      },
      options: {
        renderActions: {
          addCss: [
            200,
            function (doc) {
              const style = document.createElement('style');
              style.innerHTML = `
              .mjx-container { color: inherit; }
              mjx-container[display="true"] {
                overflow-x: auto; overflow-y: hidden; max-width: 100%;
              }
              mjx-container[display="true"]::-webkit-scrollbar { height: 6px; }
              mjx-container[display="true"]::-webkit-scrollbar-track {
                background: rgba(0,0,0,0.05); border-radius: 3px;
              }
              mjx-container[display="true"]::-webkit-scrollbar-thumb {
                background: rgba(0,0,0,0.2); border-radius: 3px;
              }
              mjx-container[display="true"]::-webkit-scrollbar-thumb:hover {
                background: rgba(0,0,0,0.3);
              }
            `;
              document.head.appendChild(style);
            },
            '',
          ],
        },
      },
    };
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Training-Free Method for Parallel Decoding of Autoregressive Models",
            "description": "This blog post investigates the possibility of parallel decoding for autoregressive models. The author notes that autoregressive and diffusion models both fundamentally model data probability distributions, and that each has advantages—autoregressive models in training and diffusion models in sampling. The goal is to achieve a training-free way to perform parallel decoding with a pretrained autoregressive model, enabling low-cost accelerated generation.",
            "published": "November 07, 2025",
            "authors": [
              
              {
                "author": "Runze Tian",
                "authorURL": "https://gua927.github.io",
                "affiliations": [
                  {
                    "name": "GenSI Lab, THU-AIR",
                    "url": "https://www.gensi-thuair.com/#/portal_home"
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Runze Tian </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Training-Free Method for Parallel Decoding of Autoregressive Models</h1> <p>This blog post investigates the possibility of parallel decoding for autoregressive models. The author notes that autoregressive and diffusion models both fundamentally model data probability distributions, and that each has advantages—autoregressive models in training and diffusion models in sampling. The goal is to achieve a training-free way to perform parallel decoding with a pretrained autoregressive model, enabling low-cost accelerated generation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#current-genmodels">Current GenModels</a> </div> <ul> <li> <a href="#autoregressive-model">Autoregressive Model</a> </li> <li> <a href="#diffusion-model">Diffusion model</a> </li> <li> <a href="#masked-diffusion-model">Masked Diffusion Model</a> </li> </ul> <div> <a href="#score-function-and-concrete-score">Score Function and Concrete Score</a> </div> <ul> <li> <a href="#score-function-and-langevin-dynamics">Score function and Langevin Dynamics</a> </li> <li> <a href="#concrete-score-and-masked-diffusion">Concrete score and masked diffusion</a> </li> </ul> <div> <a href="#ar2diff-in-consistency-space">AR2Diff in consistency space</a> </div> <div> <a href="#ar2diff-in-discrete-space">AR2Diff in discrete space</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In recent years, the rapid rise of artificial intelligence has been driven not only by advances in discriminative models but, more fundamentally, by the evolution of generative models — models that learn to represent and simulate the underlying probability distribution of data. From text and images to audio and 3D scenes, the essence of generation lies in one universal goal: to capture the complexity of real-world data distributions and to reproduce samples that are both coherent and diverse.</p> <p>However, modeling such high-dimensional and structured distributions directly is intractable. Instead of storing an explicit probability function, modern generative models encode this distribution implicitly within their network parameters and decode it through learned stochastic processes. The diversity of generative paradigms — from autoregressive (AR) models to diffusion and energy-based models — stems primarily from the different ways they design and interpret this encoding–decoding process.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-07-Note-AR2Diff/figure1-480.webp 480w,/assets/img/posts/2025-11-07-Note-AR2Diff/figure1-800.webp 800w,/assets/img/posts/2025-11-07-Note-AR2Diff/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-11-07-Note-AR2Diff/figure1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Autoregressive models, by factorizing joint probabilities into sequential conditionals, offer a simple and efficient training pipeline. Their major limitation, however, lies in the sequential nature of generation, which prevents parallel sampling. Diffusion models, in contrast, model the joint distribution directly through iterative denoising steps, enabling parallel decoding at the cost of heavier and slower training. Recent studies, such as <d-cite key="kim2025trainworstplanbest"></d-cite>, have quantitatively confirmed this asymmetry between training cost and sampling efficiency.</p> <p>This blog explores a conceptual bridge between these two paradigms — a path to train with autoregression and decode with diffusion. Since both families ultimately learn to approximate the same probability distribution, it may be possible to transfer or reinterpret an autoregressive model into a diffusion-like sampling mechanism without extensive retraining — a direction we tentatively refer to as AR2Diff. Early efforts, such as Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics (ICML 2021)<d-cite key="jayaram2021parallelflexiblesamplingautoregressive"></d-cite>, have hinted at this possibility, though the idea remained underexplored. With the resurgence of masked diffusion models (MDM) and consistency-based sampling, this line of thought now regains both practical relevance and theoretical elegance.</p> <p>In the following sections, we will review the current landscape of generative modeling, connect autoregression and diffusion through the lens of score functions, and discuss how AR2Diff might be realized in both continuous and discrete spaces.</p> <h2 id="current-genmodel">Current GenModel</h2> <h3 id="autogressive-models">Autogressive Models</h3> <p>The autoregressive model (AR) is fundamentally grounded in the chain rule decomposition of the joint distribution $p(\mathbf{x})$. Let $\mathbf{x} = (x_1, x_2, \dots, x_D) \in \mathcal{X}^D$, where $\mathcal{X}$ may denote either a discrete vocabulary or a continuous real-valued space. The model factorizes the distribution as:</p> \[\begin{equation*} p(\mathbf{x}) = \prod_{i=1}^{D}p(x_i \mid x_{&lt;i}),\quad \text{where } x_{&lt;i}:= (x_1, \dots, x_{i-1}). \end{equation*}\] <p>In parametric modeling, a neural network with parameters $\theta$ is employed to approximate each conditional distribution. For discrete data, this often takes the form of a softmax output:</p> \[p_\theta(x_i = k \mid x_{&lt;i}) = \frac{\exp(f_\theta(x_{&lt;i})_k)}{\sum_{k' \in \mathcal{V}} \exp(f_\theta(x_{&lt;i})_{k'})},\] <p>while for continuous variables, a Gaussian parameterization is common:</p> \[p_\theta(x_i \mid x_{&lt;i}) = \mathcal{N}\big(x_i; \mu_\theta(x_{&lt;i}), \sigma_\theta^2(x_{&lt;i})\big).\] <p>During training, maximum likelihood estimation (MLE) is used to minimize the negative log-likelihood:</p> \[\mathcal{L}_{\text{AR}}(\theta) = -\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \sum_{i=1}^{D} \log p_\theta(x_i \mid x_{&lt;i}) \right].\] <p>This objective is fully parallelizable under teacher forcing, as all conditioning contexts $x_{&lt;i}$ are taken from the ground-truth data. However, <strong>sampling remains inherently sequential</strong>: the generation process must proceed step-by-step,</p> \[x_i^{(s)} \sim p_\theta\big(x_i \mid x_1^{(s)}, \dots, x_{i-1}^{(s)}\big), \quad i = 1, \dots, D,\] <p>resulting in inference latency that scales linearly with the data dimensionality $D$. More fundamentally, this factorization imposes a <strong>fixed ordering</strong> $\pi$ (typically the natural index order) on the variables, despite the fact that the true data distribution $p_{\text{data}}(\mathbf{x})$ possesses no intrinsic sequential structure. While this inductive bias facilitates tractable modeling, it may constrain the model’s ability to capture non-local, symmetric, or graph-structured dependencies that do not conform to a unidirectional causal chain.</p> <h3 id="diffusion-model">Diffusion Model</h3> <p>Diffusion models construct a forward Markov chain that progressively corrupts data with noise, then learn the reverse process to enable generation. Let $\mathbf{x}<em>0 \sim p</em>{\text{data}}$. The forward process is defined as:</p> \[q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^{T} q(\mathbf{x}_t \mid \mathbf{x}_{t-1}), \quad q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1}, \beta_t \mathbf{I}),\] <p>where $\beta_t \in (0,1)$ is a pre-specified noise schedule. This process admits a closed-form expression:</p> \[\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I}),\] <p>with $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}<em>t = \prod</em>{s=1}^t \alpha_s$.</p> <p>The reverse process aims to learn a sequence of conditionals $p_\theta(\mathbf{x}<em>{t-1} \mid \mathbf{x}_t)$ such that the resulting generative chain $p</em>\theta(\mathbf{x}<em>{0:T}) = p(\mathbf{x}_T) \prod</em>{t=1}^T p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$ approximates the true data distribution. In practice, the reverse transitions are often parameterized as Gaussians, and training is performed by minimizing a variational lower bound (ELBO) or, more commonly, by direct noise regression:</p> \[\mathcal{L}_{\text{diff}}(\theta) = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \| \boldsymbol{\epsilon} - \epsilon_\theta(\mathbf{x}_t, t) \|^2 \right],\] <p>where $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\epsilon}$.</p> <p>From the perspective of score matching, diffusion models equivalently learn the time-dependent score function:</p> \[s_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t),\] <p>and sampling can be carried out by solving the reverse stochastic differential equation (SDE):</p> \[d\mathbf{x} = \left[ -\frac{1}{2} \beta(t) \left( \mathbf{x} + s_\theta(\mathbf{x}, t) \right) \right] dt + \sqrt{\beta(t)} \, d\mathbf{w},\] <p>where $\mathbf{w}$ denotes a standard Wiener process. This formulation reveals the essential nature of diffusion models: rather than explicitly modeling the density, they learn a vector field that steers samples toward high-probability regions of the data manifold.</p> <p>This implicit, geometry-aware approach endows diffusion models with remarkable expressiveness and robustness to long-range dependencies—making them especially well-suited for structured data like natural images or 3D scenes. However, this flexibility comes at a cost: training requires optimization across multiple noise levels, resulting in significantly higher computational overhead compared to autoregressive models. Moreover, the continuous-time foundation of diffusion poses fundamental challenges when applied to discrete domains (e.g., text), where gradients are undefined and semantic continuity breaks down.</p> <h3 id="masked-diffuison-model">Masked Diffuison Model</h3> <p>Masked diffusion models (MDMs)<d-cite key="shi2025simplifiedgeneralizedmaskeddiffusion"></d-cite> extend the diffusion paradigm to discrete data by replacing additive Gaussian noise with a <strong>masking mechanism</strong>. Let $\mathbf{x}_0 \in \mathcal{V}^D$ be a discrete sequence (e.g., image tokens or natural language tokens). A masking rate $\alpha_t \in [0,1]$ is defined to increase with time step $t$, for instance as $\alpha_t = 1 - (1 - \gamma)^t$. The forward process randomly masks $\lceil \alpha_t D \rceil$ positions:</p> \[\mathbf{x}_t = \mathcal{M}_t \odot \mathbf{x}_0 + (1 - \mathcal{M}_t) \odot \mathbf{1}_{[\text{MASK}]},\] <p>where $\mathcal{M}_t \in {0,1}^D$ is a binary mask vector satisfying $\mathbb{E}[|\mathcal{M}_t|_1] = (1 - \alpha_t) D$.</p> <p>The reverse process trains a unified model $p_\theta(x_i \mid \mathbf{x}_t, t)$ to predict the original token at any masked position. The training objective is typically formulated as a masked cross-entropy loss:</p> \[\mathcal{L}_{\text{MDM}}(\theta) = \mathbb{E}_{t, \mathbf{x}_0} \left[ \sum_{i: \mathcal{M}_{t,i} = 0} \log p_\theta(x_{0,i} \mid \mathbf{x}_t, t) \right].\] <p>Notably, at early timesteps (e.g., $t=1$) with high masking rates, $\mathbf{x}_1$ is nearly all <code class="language-plaintext highlighter-rouge">[MASK]</code>, forcing the model to perform “cold-start” prediction with minimal context. In contrast, as $t \to T$, $\mathbf{x}_T \approx \mathbf{x}_0$, and the task reduces to self-supervised reconstruction. This progression naturally induces a <strong>multi-scale modeling hierarchy</strong>, progressing from global structure to fine-grained detail.</p> <p>MDMs exhibit a deep formal connection to autoregressive models. In the limiting case where the masking strategy is fixed to “mask only the last position,” the MDM exactly recovers a standard AR model. Conversely, under fully random masking, the model must learn the conditional distribution over <strong>any subset</strong> of variables given the rest—that is, for any $\mathcal{S} \subset {1,\dots,D}$, it implicitly learns $p(\mathbf{x}<em>{\mathcal{S}^c} \mid \mathbf{x}</em>{\mathcal{S}})$. This capability far exceeds the unidirectional conditioning of AR models, yet the training objective retains the same semantic essence: <em>predicting missing information from partial observations</em>.</p> <p>It is precisely this <strong>semantic equivalence</strong> coupled with <strong>structural disparity</strong> that provides a theoretical foundation for AR2Diff: if an autoregressive model has already internalized rich contextual dependencies through sequential training, can we reinterpret it as a denoiser in a mask-based iterative refinement loop, thereby enabling parallel sampling without retraining? The answer may lie in the dual relationship between conditional probabilities and score functions—an insight we will explore in the next section.</p> <h2 id="score-function-and-concrete-score">Score Function and Concrete Score</h2> <h3 id="score-function-and-langevin-dynamics">Score Function and Langevin Dynamics</h3> <h3 id="concrete-score">Concrete Score</h3> <h2 id="ar2diff-in-consistency-space">AR2Diff in consistency space</h2> <h2 id="ar2diff-in-discrete-space">AR2Diff in discrete space</h2> <h2 id="conclusion">Conclusion</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/papers.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Note-FM/">Flow Matching and Continuous Normalizing Flows</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Note-Diffusion-DDPM-and-NCSN/">The Unification of DDPM and Score-based Models</a> </li> <br> <br> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 R. Z. Tian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?e510a9b2408214d7071a1c65feace861"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>