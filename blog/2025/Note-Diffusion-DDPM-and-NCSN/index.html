<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Unification of DDPM and Score-based Models | Runze Tian </title> <meta name="author" content="R. Z. Tian"> <meta name="description" content="This post explores the unification of DDPM and Score-based Models in diffusion generative modeling. We show how x-prediction and score-prediction are fundamentally equivalent, and how both can be viewed through the lens of Stochastic Differential Equations (SDEs)."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/moonshot.png?ac494055d0715958a4a25e163d4405f7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gua927.github.io/blog/2025/Note-Diffusion-DDPM-and-NCSN/"> <script src="/assets/js/theme.js?8a9658bcfd3c4d23968422c4b9dc98ec"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script>
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)'],
        ],
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]'],
        ],
        processEscapes: true,
        processEnvironments: true,
        processRefs: true,
        digits: /^(?:[0-9]+(?:\{,\}[0-9]{3})*(?:\.[0-9]*)?|\.[0-9]+)/,
        maxBuffer: 5 * 1024,
      },
      chtml: {
        scale: 1,
        minScale: 0.5,
        matchFontHeight: true,
        linebreaks: { automatic: false, width: 'container' },
      },
      startup: {
        pageReady: () => {
          return MathJax.startup.defaultPageReady().then(() => {
            console.log('MathJax typesetting complete');
          });
        },
      },
      options: {
        renderActions: {
          addCss: [
            200,
            function (doc) {
              const style = document.createElement('style');
              style.innerHTML = `
              .mjx-container { color: inherit; }
              mjx-container[display="true"] {
                overflow-x: auto; overflow-y: hidden; max-width: 100%;
              }
              mjx-container[display="true"]::-webkit-scrollbar { height: 6px; }
              mjx-container[display="true"]::-webkit-scrollbar-track {
                background: rgba(0,0,0,0.05); border-radius: 3px;
              }
              mjx-container[display="true"]::-webkit-scrollbar-thumb {
                background: rgba(0,0,0,0.2); border-radius: 3px;
              }
              mjx-container[display="true"]::-webkit-scrollbar-thumb:hover {
                background: rgba(0,0,0,0.3);
              }
            `;
              document.head.appendChild(style);
            },
            '',
          ],
        },
      },
    };
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Unification of DDPM and Score-based Models",
            "description": "This post explores the unification of DDPM and Score-based Models in diffusion generative modeling. We show how x-prediction and score-prediction are fundamentally equivalent, and how both can be viewed through the lens of Stochastic Differential Equations (SDEs).",
            "published": "November 03, 2025",
            "authors": [
              
              {
                "author": "Runze Tian",
                "authorURL": "https://gua927.github.io",
                "affiliations": [
                  {
                    "name": "GenSI Lab, THU-AIR",
                    "url": "https://www.gensi-thuair.com/#/portal_home"
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Runze Tian </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Unification of DDPM and Score-based Models</h1> <p>This post explores the unification of DDPM and Score-based Models in diffusion generative modeling. We show how x-prediction and score-prediction are fundamentally equivalent, and how both can be viewed through the lens of Stochastic Differential Equations (SDEs).</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#ddpm-from-a-score-perspective">DDPM from a Score Perspective</a> </div> <div> <a href="#sde-model">SDE Model</a> </div> <ul> <li> <a href="#definition">Definition</a> </li> <li> <a href="#forward-sde">Forward SDE</a> </li> <li> <a href="#reverse-sde">Reverse SDE</a> </li> <li> <a href="#optimization-target">Optimization Target</a> </li> <li> <a href="#pc-sampling">PC Sampling</a> </li> </ul> <div> <a href="#probability-flow-ode">Probability Flow ODE</a> </div> <div> <a href="#conditional-generation">Conditional Generation</a> </div> </nav> </d-contents> <h2 id="ddpm-from-a-score-perspective">DDPM from a Score Perspective</h2> <p>In <strong><em>DDPM</em></strong> <d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite>, we know that</p> \[x_t\sim q(x_t|x_0) = N(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I) \tag{1}\] <p>According to <strong><em>Tweedie’s Formula</em></strong>, we can obtain:</p> \[\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0 = \boldsymbol{x}_t + (1 - \bar{\alpha}_t) \nabla_{x_t} \log p(\boldsymbol{x}_t) \tag{2}\] <blockquote> <p><strong><em>Tweedie’s Formula:</em></strong></p> <p>For a Gaussian variable $z\sim \mathcal N(z;\mu_z,\Sigma_z)$, we have</p> \[\mu_z=z+\Sigma_z\nabla_z\log p(z)\] </blockquote> <p>Meanwhile, from (1) we know</p> \[x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon_t\] <p>Substituting into (2), we obtain</p> \[\nabla_{x_t}\log p(x_t)=-\frac{\epsilon_t}{\sqrt{1-\bar{\alpha}_t}}\] <p>Thus</p> \[\begin{align*} \boldsymbol{\mu}_q &amp;= \frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}} \boldsymbol{\varepsilon}_t\\ &amp;=\frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}} \color{red}\nabla_{x_t} \log p(\boldsymbol{x}_t) \end{align*}\] <p>Similarly, we model the reverse process as</p> \[\begin{align*} \boldsymbol{\mu}_{\theta} &amp;= \frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}} \boldsymbol{\varepsilon}_{\theta}(x_t,t)\\ &amp;=\frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}} \color{red}s_{\theta}(x_t,t) \end{align*}\] <p>Therefore, we transform the estimation of $\epsilon_t$ and $\epsilon_{\theta}$ in <strong><em>DDPM</em></strong> into the estimation of $\nabla\log p(x)$, which gives</p> \[\begin{align*} &amp;\arg\min_{\theta} D_{\text{KL}} \left( q(\boldsymbol{x}_{t-1} \vert \boldsymbol{x}_t, \boldsymbol{x}_0) \parallel p_{\theta}(\boldsymbol{x}_{t-1} \vert \boldsymbol{x}_t) \right) \\ =&amp; \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \lVert \boldsymbol{\mu}_{\theta} - \boldsymbol{\mu}_q \rVert_2^2 \right] \\ =&amp; \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\lVert \frac{1 - \alpha_t}{\sqrt{\alpha_t}} s_{\theta}(\boldsymbol{x}_t, t) - \frac{1 - \alpha_t}{\sqrt{\alpha_t}} \nabla \log p(\boldsymbol{x}_t) \right\rVert_2^2 \right] \\ =&amp; \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \frac{(1 - \alpha_t)^2}{\alpha_t} \left[ \color{red}\lVert s_{\theta}(\boldsymbol{x}_t, t) - \nabla \log p(\boldsymbol{x}_t) \rVert_2^2 \color{black}\right] \end{align*}\] <p>Hence, we find that the optimization objective of DDPM is actually consistent with <strong><em>Score-Based Models</em></strong>, both estimating the <strong><em>score function</em></strong>.</p> <p>We further compare the optimization objective of DDPM:</p> \[L_t = \mathbb{E} \left[ \frac{(1 - \alpha_t)^2}{2\alpha_t (1 - \bar{\alpha}_t) \sigma^2} \color{red}\left\| \boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta \left( x_t, t \right) \right\|_2^2 \color{black}\right]\] <p>We note that when $L_t$ reaches its optimum, we have</p> \[\epsilon_{\theta}(x_t,t)=\mathbb E\big[\epsilon_t|x_t,t\big]=\mathbb E\big[\epsilon_t|x_0,t\big]\] <p>This indicates that the model actually learns the mean of the noise given data $x_0$. In other words, the conditional expectation learned by our network already contains information about the true sample $x_0$, which is what enables us to obtain the distribution of true data by learning the noise.</p> <p>Meanwhile, we find that under optimal conditions, $L_t$ cannot reach 0, meaning that our optimization objective does not achieve maximum likelihood between the predicted distribution and the true distribution (otherwise the loss should reduce to 0). Combined with <strong><em>Score-Based Models</em></strong>, we know this is because we are actually predicting the <strong><em>score</em></strong> of the data distribution, and there is still a certain gap between the score distribution and the data distribution.</p> <hr> <h2 id="sde-model">SDE Model</h2> <p>What happens if we extend the finite steps $T$ to infinite steps? Experimental validation shows that larger $T$ can yield more accurate likelihood estimates and better quality results. Thus, continuous-time perturbation of data can be modeled as a stochastic differential equation (SDE).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure1-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure1-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="definition">Definition</h3> <p>There are many forms of SDEs. One form given by Dr. Yang Song in his paper )<d-cite key="song2021scorebasedgenerativemodelingstochastic"></d-cite> (which can be considered a Diffusion-Type SDE, requiring coefficients to depend only on time <code class="language-plaintext highlighter-rouge">t</code> and current value <code class="language-plaintext highlighter-rouge">x</code>) is:</p> \[\mathrm{d}\boldsymbol{x} = f(\boldsymbol{x}, t)\mathrm{d}t + g(t)\mathrm{d}\boldsymbol{w}\] <p>where $f(\cdot)$ is called the drift coefficient, $g(t)$ is called the diffusion coefficient, $\boldsymbol{w}$ is a standard Brownian motion, and $\mathrm{d}\boldsymbol{w}$ can be viewed as white noise. The solution to this stochastic differential equation is a set of continuous random variables $\lbrace\boldsymbol x(t)\rbrace_{t\in [0,T]}$, where $t$ represents the continuous version of the discrete form $(1, 2, \ldots, T)$.</p> <p>We use $p_t(\boldsymbol{x})$ to denote the probability density function of $\boldsymbol{x}(t)$, which corresponds to the previous $p_{\sigma_t}(\boldsymbol{x}_t)$. Here $p_0(\boldsymbol{x}) = p(\boldsymbol{x})$ is the original data distribution, and $p_T(\boldsymbol{x}) = \mathcal{N}(0, \mathbf{I})$ is the white noise obtained after noise perturbation.</p> <blockquote> <p><strong><em>Brownian Motion</em></strong></p> <p>If a stochastic process $\lbrace X(t), t \geq 0\rbrace$ satisfies:</p> <ul> <li>$X(t)$ is an independent increment process;</li> <li>$\forall s, t &gt; 0, X(s + t) - X(s) \sim N(0, c^2 t)$</li> </ul> <p>then the stochastic process $\lbrace X(t), t \geq 0\rbrace$ is called <strong>Brownian motion</strong> (denoted as $B(t)$) or <strong>Wiener process</strong> (denoted as $W(t)$). In this text, we will subsequently denote it as $W(t)$. If $c = 1$, it is called <strong>standard Brownian motion</strong>, satisfying $W(t) \sim N(0, t)$.</p> </blockquote> <h3 id="forward-sde">Forward SDE</h3> <p>We can directly discretize the equation</p> \[\mathrm{d}\boldsymbol{x} = f(\boldsymbol{x}, t)\mathrm{d}t + g(t)\mathrm{d}\boldsymbol{w}\] <p>where</p> \[dx\to x_{t+\Delta t}-x_t\\ dt\to \Delta t\\ dw\to w(t+\Delta t)-w(t)\sim\mathcal N(0,\Delta t)=\sqrt{\Delta t}\epsilon\] <p>Thus, the discrete form of the SDE is represented as:</p> \[\color{red} x_{t+\Delta t}-x_t=f(x,t)\Delta t+g(t)\sqrt{\Delta t}\epsilon\] <p>where $\epsilon\sim\mathcal N(0,1)$.</p> <h4 id="ve-sde">VE-SDE</h4> <p>For NCSN, the forward process (adding noise) is shown as follows:</p> \[x_t=x_0+\sigma_t\epsilon\\ x_{t+\Delta t}=x_t+\underbrace{\sqrt{\sigma_{t+\Delta t}^2-\sigma_t^2}}_{\color{red}\sqrt{\frac{\sigma_{t+\Delta t}^2-\sigma_t^2}{\Delta t}}\sqrt{\Delta t}}\epsilon\] <p>Therefore, in the corresponding SDE representation</p> \[f(x_t,t)=0\\ g(t)=\lim\limits_{\Delta\to 0}\sqrt{\frac{\sigma_{t+\Delta t}^2-\sigma_t^2}{\Delta t}}=\sqrt{2\sigma_t\dot\sigma_t}\] <p>The corresponding continuous <strong>SDE</strong> for the <strong>VE</strong> process is:</p> \[\color{blue}dx=\sqrt{2\sigma_t\dot\sigma_t}dW\] <h4 id="vp-sde">VP-SDE</h4> <p>For DDPM, its forward process is represented as follows:</p> \[x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon\\ x_{t+1}=\sqrt{1-\beta_{t+1}}x_t+\sqrt{\beta_{t+1}}\epsilon\] <p>We continualize the discrete time $1,2,\cdots,t,\cdots T$ to $[0,1]$, i.e., let</p> \[t\to\frac{t}{T}=t'\\ 1\to\frac{1}{T}=\Delta t\] <p>We can also let $\beta_{t’}=T\beta_t$, thus</p> \[\begin{align*} x_{t'+\Delta t}&amp;=x_{t+1}=\sqrt{1-\beta_{t+1}}x_t+\sqrt{\beta_{t+1}}\epsilon\\ &amp;=\sqrt{1-\beta_{t'+\Delta t}\Delta t}\cdot x_{t'}+\sqrt{\beta_{t'+\Delta t}\Delta t}\cdot \epsilon\\ &amp;=\big(1-\frac{1}{2}\beta_{t'}\Delta t\big)x_{t'}+\sqrt{\beta_{t'}}\sqrt{\Delta t}\epsilon \end{align*}\] <p>Therefore, in the corresponding SDE representation, we have</p> \[f(x_t,t)=-\frac{1}{2}\beta_{t'}x_t\\ g(t)=\sqrt{\beta_{t'}}\] <p>The corresponding continuous <strong>SDE</strong> for the <strong>VP</strong> process is:</p> \[\color{blue}dx=-\frac{1}{2}\beta_{t'}x_tdt+\sqrt{\beta_{t'}}dW\] <p>We expect that when $t\to T$, the image becomes pure noise, then $\sigma_t\to\infty$, but $\bar{\alpha}_t\to 0$ is sufficient. This requires that in <strong>NCSN</strong>, the variance of noise gradually expands, while in <strong>DDPM</strong>, the noise variance remains between $(0,1)$. Therefore, they are respectively called <strong>VE-SDE</strong> and <strong>VP-SDE</strong>.</p> <h3 id="reverse-sde">Reverse SDE</h3> <p>Using the discrete forward <strong>SDE</strong>, we can derive its reverse process. From the forward <strong>SDE</strong>:</p> \[\color{red} x_{t+\Delta t}-x_t=f(x,t)\Delta t+g(t)\sqrt{\Delta t}\epsilon\] <p>we have the conditional probability:</p> \[x_{t+\Delta t}|x_t\sim \mathcal{N}(x_t+f(x_t,t)\Delta t, g^2(t)\Delta tI)\] <p>Considering the reverse process from $x_{t+\Delta t}$ to $x_t$, we have the conditional probability:</p> \[\begin{align*} p(x_t|x_{t+\Delta t})&amp;=\frac{p(x_{t+\Delta t}|x_t)p(x_t)}{p(x_{t+\Delta t})}\\ &amp;=p(x_{t+\Delta t}|x_t)\exp(\log p(x_t)-\log p(x_{t+\Delta t}))\\ &amp;\approx p(x_{t+\Delta t}|x_t)\exp\{\color{red} -(x_{t+\Delta t}-x_t)\nabla_{x_t}\log p(x_t)-\Delta t\frac{\partial}{\partial t}\log p(x_t) \color{black}\}\\ &amp;\propto \exp\{ -\frac{\|x_{t+\Delta t}-x_t-f(x_t,t)\Delta t\|_2^2}{2g^2(t)\Delta t} - (x_{t+\Delta t}-x_t)\nabla_{x_t}\log p(x_t)-\Delta t\frac{\partial}{\partial t}\log p(x_t) \}\\ &amp;=\exp\bigg\{ -\frac{1}{2g^2(t)\Delta t}\|(x_{t+\Delta t}-x_t)-\big[f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \big]\Delta t\|_2^2 - \Delta t\frac{\partial}{\partial t}\log p(x_t)-\frac{f^2(x_t,t)\Delta t}{2g^2(t)}+\frac{\|f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \|_2^2\Delta t}{2g^2(t)} \bigg\}\\ &amp;\stackrel{\Delta t \to 0}{=} \exp\bigg\{ -\frac{1}{2g^2(t)\Delta t}\| (x_{t+\Delta t}-x_t)-\big[f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \big]\Delta t \|_2^2 \bigg\} \end{align*}\] <p>Therefore, $x_t|x_{t+\Delta t}$ follows a Gaussian distribution with mean and variance as follows:</p> \[\mu=x_{t+\Delta t}-\big[f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \big]\Delta t\\ \sigma^2=g^2(t)\Delta t\] <p>Thus, we can obtain both the discrete and continuous forms of the reverse SDE process:</p> \[x_{t+\Delta t}-x_t=\big[f(x_t+\Delta t,t+\Delta t)-g^2(t+\Delta t)\nabla_{x_t+\Delta t}\log p(x_{t+\Delta t}) \big]\Delta t+g(t+\Delta t)\sqrt{\Delta t}\epsilon\] \[\color{blue}dx=\big[f(x_t,t)-g^2(t)\color{red}\nabla_{x_t}\log p(x_t)\color{blue} \big]dt+g(t)dW\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure2-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure2-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Therefore, after we have learned the <strong><em>score function</em></strong>, the reverse process becomes completely solvable. During generation, we start by sampling $x_T \sim \mathcal{N}(0, 1)$, and gradually obtain $x_0$ using the discrete process above. This discretization method for stochastic differential equations is also called the <strong>Euler–Maruyama method</strong>.</p> <p>For NCSN, its forward VE process continuous SDE is:</p> \[\color{blue}dx=\sqrt{2\sigma_t\dot\sigma_t}dW\] <p>Then its reverse process is:</p> \[dx=-2\sigma_t\dot\sigma_t\nabla_{x_t}\log p(x_t)dt+\sqrt{2\sigma_t\dot\sigma_t}dW\] <p>Written in discrete form, it becomes</p> \[x_t-x_{t-1}=-2\sigma_t\dot\sigma_t\nabla_{x_t}\log p(x_t)\Delta t+\sqrt{2\sigma_t\dot\sigma_t}\sqrt{\Delta t}\epsilon_t\] <p>In the case where $\sigma_t\dot\sigma_t=1$, if we set $\Delta t=\delta$ (this is done only for formal consistency, without real meaning), then we have</p> \[x_{t-1}=x_t+2\delta\nabla_{x_t}\log p(x_t)+\sqrt{2\delta}\epsilon\] <p>This also unifies with the <strong><em>Langevin Equation</em></strong> mentioned earlier.</p> <h3 id="optimization-target">Optimization Target</h3> <p>Solving the reverse SDE requires us to know the terminal distribution $p_T(\boldsymbol{x})$ and the score function $\nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})$. By design, the former is close to the prior distribution $\pi(\boldsymbol{x})$, which is fully tractable.</p> <p>To estimate $\nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})$, we train a <strong>time-dependent score-based model</strong> $s_\theta(\boldsymbol{x}, t)$ such that $s_\theta(\boldsymbol{x}, t) \approx \nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})$. This is similar to NCSN’s $s_\theta(\boldsymbol{x}, i)$, which after training satisfies $s_\theta(\boldsymbol{x}, i) \approx \nabla_{\boldsymbol{x}} \log p_{\sigma_i}(\boldsymbol{x})$.</p> <p>Our training objective for $s_\theta(\boldsymbol{x}, t)$ is a continuous weighted combination of Fisher divergences, given by <d-cite key="song2019slicedscorematchingscalable"></d-cite>:</p> \[\mathbb{E}_{t \in \mathcal{U}(0,T)} \mathbb{E}_{p_t(\mathbf{x})} \left[ \lambda(t) \left\| \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, t) \right\|_2^2 \right]\] <h4 id="relationship-with-likelihood">Relationship with Likelihood</h4> <p>When $\lambda(t)=g^2(t)$, under some regularity conditions, there exists an important connection between our weighted combination of Fisher divergences and the KL divergence from $p_0$ to $p_\theta$:</p> \[\begin{align*} \mathrm{KL}(p_0(\mathbf{x}) \| p_\theta(\mathbf{x})) &amp;\leq \frac{T}{2} \mathbb{E}_{t \in \mathcal{U}(0,T)} \mathbb{E}_{p_t(\mathbf{x})} \left[ \lambda(t) \| \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, t) \|_2^2 \right] \\ &amp;+ \mathrm{KL}(p_T \| \pi) \end{align*}\] <p>Due to this special connection with KL divergence, and the equivalence between minimizing KL divergence and maximizing likelihood in model training, we call $\lambda(t)=g(t)^2$ the <strong>likelihood weighting function</strong>. Using this likelihood weighting function, we can train score-based generative models to achieve very high likelihoods.</p> <h3 id="pc-sampling">PC Sampling</h3> <p>Reviewing DDPM and NCSN from an algorithmic implementation perspective, DDPM is based on the Markov assumption, assuming that samples at different times follow conditional probability distributions. Therefore, DDPM uses Ancestral Sampling to solve the SDE equation, with the algorithm shown below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure3-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure3-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>While NCSN relies on Langevin Dynamics for iterative optimization under the same noise distribution. For different noise magnitudes, there is no dependency relationship between the obtained samples. Its sampling method is shown as follows:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure4-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure4-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The former can be seen as solving the discrete form of the SDE equation, called the Predictor, while the latter can be seen as a further optimization process, called the Corrector. The author combines these two parts to present the Predictor-Corrector Sampling Method:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure5-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure5-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <h2 id="probability-flow-ode">Probability Flow ODE</h2> <p>We can transform any <strong><em>SDE</em></strong> into an <strong><em>ODE</em></strong> without changing the marginal distributions $\lbrace p_t(x)\rbrace_{t\in[0,T]}$ of the stochastic differential equation. Therefore, by solving this <strong><em>ODE</em></strong>, we can sample from the same distribution as the <strong><em>Reverse SDE</em></strong>. The <strong><em>ODE</em></strong> corresponding to the <strong><em>SDE</em></strong> is called the <strong><em>Probability flow ODE</em></strong>, with the form:</p> \[\color{blue}dx=\big[f(x_t,t)-\frac{1}{2}g^2(t)\color{red}\nabla_{x_t}\log p(x_t)\color{blue} \big]dt\] <p>The figure below depicts trajectories of stochastic differential equations (SDEs) and probability flow ordinary differential equations (ODEs). It can be seen that ODE trajectories are significantly smoother than SDE trajectories, and they transform the same data distribution into the same prior distribution and vice versa, sharing the same set of marginal distributions $\lbrace p_t(\boldsymbol{x})\rbrace_{t\in[0,T]}$. In other words, trajectories obtained by solving the probability flow ODE have the same marginal distributions as SDE trajectories.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure6-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure6-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure6.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Using <strong><em>probability flow ODEs</em></strong> provides many benefits:</p> <ol> <li>Exact likelihood computation</li> <li>Manipulating latent representations</li> <li>Uniquely identifiable encoding</li> <li>Efficient sampling</li> </ol> <hr> <h2 id="conditional-generation">Conditional Generation</h2> <p>According to Bayes’ theorem</p> \[p(x|y)=\frac{p(x)p(y|x)}{p(y)}\] <p>Taking the score with respect to $x$ on both sides gives</p> \[\nabla_x\log p(x|y)=\nabla_x\log p(x)+\nabla_x\log p(y|x)\] <p>Both latter terms are <strong><em>score functions</em></strong> that we can estimate. Therefore, we can generate $p(x|y)$ by solving the reverse SDE.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Note-AR2Diff/">Training-Free Method for Parallel Decoding of Autoregressive Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Note-FM/">Flow Matching and Continuous Normalizing Flows</a> </li> <br> <br> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 R. Z. Tian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?e510a9b2408214d7071a1c65feace861"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>