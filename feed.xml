<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gua927.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gua927.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-05T13:14:00+00:00</updated><id>https://gua927.github.io/feed.xml</id><title type="html">Runze Tian</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Flow Matching and Continuous Normalizing Flows</title><link href="https://gua927.github.io/blog/2025/Note-FM/" rel="alternate" type="text/html" title="Flow Matching and Continuous Normalizing Flows"/><published>2025-11-05T12:00:00+00:00</published><updated>2025-11-05T12:00:00+00:00</updated><id>https://gua927.github.io/blog/2025/Note-FM</id><content type="html" xml:base="https://gua927.github.io/blog/2025/Note-FM/"><![CDATA[<h2 id="overview">Overview</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-5-Note-FM/figure1-480.webp 480w,/assets/img/posts/2025-11-5-Note-FM/figure1-800.webp 800w,/assets/img/posts/2025-11-5-Note-FM/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-5-Note-FM/figure1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Flow-based Models</strong> are generative models based on <strong>Normalizing Flows (NFs)</strong>, which transform complex probability distributions into simple ones through a series of probability density function transformations, and generate new data samples through inverse transformations. <strong>Continuous Normalizing Flows (CNFs)</strong> extend <strong>Normalizing Flows</strong> by using ordinary differential equations (<strong>ODEs</strong>) to represent continuous transformation processes for modeling probability distributions.</p> <p><strong>Flow Matching (FM)</strong> is a method for training <strong>Continuous Normalizing Flows</strong> that trains the model by learning <strong>Vector Fields</strong> associated with probability paths, and uses <strong>ODE</strong> solvers to generate new samples.</p> <p>Diffusion models are a special case of <strong>Flow Matching</strong> applications. Using FM can improve their training stability. Furthermore, constructing probability paths using <strong><em>Optimal Transport</em></strong> techniques can further accelerate training speed and enhance model generalization ability.</p> <h2 id="normalizing-flows">Normalizing Flows</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-5-Note-FM/figure2-480.webp 480w,/assets/img/posts/2025-11-5-Note-FM/figure2-800.webp 800w,/assets/img/posts/2025-11-5-Note-FM/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-5-Note-FM/figure2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Normalizing Flows (NFs)</strong> are invertible probability density transformation methods whose core idea is to progressively transform a simple distribution (typically a Gaussian distribution) into a complex target distribution through a series of invertible transformation functions. <strong>This process can be viewed as an iterative sequence of variable substitutions, where each substitution follows the change of variables principle for probability density functions</strong>. Through this approach, <strong>Normalizing Flows</strong> can precisely compute the probability density of the transformed distribution, thereby achieving an exact mapping from simple to complex distributions.</p> <p>Let $p_0(\mathbf{z_0})$ be the original simple distribution (e.g., standard Gaussian distribution). <strong><em>Normalizing Flows</em></strong> aim to transform it into the target distribution $p(x)$ through a series of invertible transformations $f_i$. These transformations define a mapping from $z_0$ to $x$, and each transformation $f_i$ has its inverse transformation $f_i^{-1}$. Thus, the transformation process can be represented as:</p> \[x=z_K=f_K\circ f_{K-1}\circ\cdots\circ f_1(z_0)\] <p>For the $i$-th step, we have:</p> \[\mathbf{z}_{i-1} \sim p_{i-1}(\mathbf{z}_{i-1})\\ \mathbf{z}_i = f_i(\mathbf{z}_{i-1}), \quad \text{thus } \mathbf{z}_{i-1} = f_i^{-1}(\mathbf{z}_i)\] <p>According to the change of variables formula for probability density functions, we obtain:</p> \[\begin{align*} p_i(\mathbf{z}_i) &amp;= p_{i-1}(f_i^{-1}(\mathbf{z}_i)) \left| \det \left( \frac{d f_i^{-1}}{d \mathbf{z}_i} \right) \right|\\ &amp;= p_{i-1}(\mathbf{z}_{i-1}) \left| \det \left( \frac{d f_i}{d \mathbf{z}_{i-1}} \right)^{-1} \right|\\ &amp;= p_{i-1}(\mathbf{z}_{i-1}) \left| \det \left( \frac{d f_i}{d \mathbf{z}_{i-1}} \right) \right|^{-1} \end{align*}\] <p>The log-likelihood is given by:</p> \[\log p_i(x)=\log p_{i-1}(z_{K-1})-\log\bigg|\det\frac{df_i}{dz_{i-1}}\bigg|\] <p>Thus we have</p> \[\log p(x)=\log \pi_0(z_0)-\sum\limits_{i=1}^{K}\log\bigg|\det\frac{df_i}{dz_{i-1}} \bigg|\] <p>When this series of transformation functions $f_i$ are invertible and the Jacobian matrices are tractable to compute, during model training, the optimization objective is the negative log-likelihood:</p> \[\mathcal L(\mathcal D)=-\frac{1}{\mathcal D}\sum_{x\in\mathcal D}\log p(x)\] <h2 id="expressive-power-of-flow-based-models">Expressive Power of Flow-Based Models</h2> <p>We consider whether we can transform a simple distribution $p(u)$ into any arbitrary probability distribution $p(x)$. Assume $x$ is a $D$-dimensional vector with $p(x)&gt;0$, and the probability distribution of $x_i$ depends only on the previous elements $x_{1:i-1}$. Then we can decompose $p_x(x)$ as a product of conditional probabilities</p> \[p_x(x)=\prod_{i=1}^{D}p_x(x_i|x_{1:i-1})\] <p>Assume the transformation $F$ maps $x$ to $z$, where the value of $z_i$ is determined by the cumulative distribution function (CDF) of $x_i$</p> \[z_i=F_i(x_i,x_{1:i-1})=\int_{-\infty}^{x_i}p_x(x_i'|x_{1:i-1})dx'_i=P(x'_i\le x_i|x_{1:i-1})\] <p>Clearly, $F_i$ is differentiable, and its partial derivative with respect to $x_i$ equals $p_x(x_i|x_{1:i-1})$. Since the partial derivative of $F_i$ with respect to $x_j~(j&gt;i)$ is 0, $J_F(x)$ is a lower triangular matrix, and thus its determinant equals the product of its diagonal elements, i.e.,</p> \[\det J_F(x)=\prod_{i=1}^{D}\frac{\partial F_i}{\partial x_i}=\prod_{i=1}^{D}p_x(x_i|x_{1:i-1})=p_x(x)&gt;0\] <p>Since $p_x(x)&gt;0$, the determinant is also greater than zero, thus the inverse of transformation $F$ must exist. Therefore, we have</p> \[p_z(z)=p_x(x)|\det J_F(x)|^{-1}=1\] <p>i.e., $z$ follows a uniform distribution $[0,1]^D$ in $D$-dimensional space.</p> <p>From the above discussion, we can see that we can transform any distribution into a uniform distribution, and also transform a uniform distribution into any distribution. Thus, through normalizing flows, we can achieve mutual transformation between any two distributions.</p> <h2 id="continuous-normalizing-flows">Continuous Normalizing Flows</h2> <p><strong>Continuous Normalizing Flows (CNFs)</strong> are an extension of <strong>Normalizing Flows</strong> that can better model complex probability distributions. In traditional <strong>Normalizing Flows</strong>, transformations are typically defined through a series of <strong>invertible discrete functions</strong>, whereas in <strong>CNFs</strong>, these transformations are <strong>continuous</strong>, enabling the model to adapt more smoothly to data distributions and enhancing the model’s expressive power.</p> <p>In the continuous setting, <strong><em>FM</em></strong> can be formalized as follows:</p> <ul> <li> <p><strong><em>Trajectory</em></strong></p> <p>A trajectory is a mapping from time to sample position. The input is $t$, and the output is $X_t$. The domain of $t$ is $[0,1]$, and the domain of $X_t$ is $\mathbb R^d$</p> \[X:[0,1]\to \mathbb R^d,\quad t\to X_t\] <p>At $t=0$, $X_0$ comes from a simple initial distribution $p_{init}$. At time $t=1$, we want $X_1$ to follow the true data distribution $p_{data}$.</p> </li> <li> <p><strong><em>Vector Field</em></strong></p> <p>A vector field is a mapping from position and time to instantaneous velocity. The inputs are <strong><em>location</em></strong> and <strong><em>time</em></strong>, and the output is <strong><em>velocity</em></strong>.</p> \[u:\mathbb R^d\times[0,1]\to \mathbb R^d,\quad (x,t)\to u_t(x)\] <p>At any position $x$ and any time $t$, the vector field provides the direction and speed of sample transformation.</p> </li> <li> <p><strong><em>ODE</em></strong></p> <p>The trajectory $X_t$ must satisfy the following initial value problem:</p> \[\frac{d}{dt}X_t=u_t(X_t),\quad X_0=x_0\] <p>Here, $x_0\sim p_{init}$ is a sample from the initial simple distribution.</p> <p>Solving an ordinary differential equation means finding a curve $X_t$ such that its tangent velocity at each time $t$ exactly equals the value of the vector field $u_t$ at its position (i.e., the derivative of $X_t$ equals $u_t$). That is, solving this differential equation yields the path starting from $x_0$.</p> </li> <li> <p><strong><em>Flow</em></strong></p> <p>A flow is a mapping of the evolution of the initial point $x_0$ at each time, denoted as</p> \[\psi_t(x_0)=X_t\] <p>with domain $\psi:\mathbb R^d\times[0,1]\to \mathbb R^d,\quad (x_0,t)\to X_t$</p> <ul> <li> <p>The flow tells us the position of the sample at time $t$ starting from $X_0=x_0,t=0$, i.e., what $X_t$ is.</p> </li> <li> <p>The flow is the solution satisfying the above <strong><em>ODE</em></strong>: \(\frac{d}{dt}\psi_t(x_0)=u_t(\psi_t(x_0)),~\psi_0(x_0)=x_0\)</p> </li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-5-Note-FM/figure3-480.webp 480w,/assets/img/posts/2025-11-5-Note-FM/figure3-800.webp 800w,/assets/img/posts/2025-11-5-Note-FM/figure3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-5-Note-FM/figure3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In practice, when using <strong><em>CNFs</em></strong>, we need to first estimate the vector field $u_t(x)$, which can be approximated by a neural network $u_t^{\theta}$. After obtaining the approximation $u_t^{\theta}$, we solve the corresponding <strong><em>ODE</em></strong> to get the trajectory $\psi_t^{\theta}(X_0)$.</p> <p>We want the sample distribution to exactly match $p_{data}$ when the model evolves to the endpoint $t=1$:</p> \[X_1=\psi_1^{\theta}(X_0)\sim p_{data}\] <p>Generally, we train $\theta$ through <strong><em>Flow Matching</em></strong> to make the evolved distribution consistent with the data distribution, i.e., to have the endpoint $X_1$ of the <strong><em>trajectory</em></strong> follow the $p_{data}$ distribution.</p> <p>After learning the vector field $u_t^{\theta}$, we need to numerically solve the $ODE$ (using first-order Euler method) to obtain $X_t$:</p> \[X_{t+h}=X_t+hu_t^{\theta}(X_t)\] <p>Below, we will focus on introducing the <strong><em>Flow Matching</em></strong> technique.</p> <h2 id="flow-matching">Flow Matching</h2> <p>An intuitive method for training <strong>Continuous Normalizing Flows</strong> is to obtain the distribution of $x_1$ by solving the <strong>ODE</strong> given initial condition $x_0$, and then constrain it to match the true data distribution using a divergence minimization measure (such as KL divergence). However, since intermediate trajectories are numerous and unknown, inferring $x_1$ (through sampling or computing likelihood) requires repeated <strong>ODE</strong> simulations, resulting in enormous computational cost. To address this, the paper proposes a new method called <strong>Flow Matching (FM)</strong>.</p> <p><strong>Flow Matching</strong> is a technique suitable for training <strong>Continuous Normalizing Flows</strong> that is <strong>Simulation-Free</strong>, meaning it does not require ODE inference of the target data distribution. <strong>Its core idea is to ensure that the dynamic characteristics between the model-predicted vector field and the vector field describing the actual motion of data points remain consistent, thereby ensuring that the final probability distribution obtained through CNFs transformation matches the expected target distribution.</strong></p> <p>Specifically, given a target probability density path $p_t(x)$ and its corresponding vector field $u_t(x)$, where the probability density path $p_t(x)$ is generated by this vector field $u_t(x)$, and $v_t(x)$ is the vector field to be learned, the <strong>Flow Matching</strong> optimization objective can be defined as:</p> \[\color{red} \mathcal L_{FM}(\theta)=\mathbb E_{t\sim U[0,1],x\sim p_t(x)}\|v_t(x)-u_t(x)\|^2\] <p>The core of the <strong>Flow Matching</strong> objective is to minimize this loss function so that its predicted vector field $v_t(x)$ is as close as possible to the actual vector field $u_t(x)$, thereby accurately generating the target probability density path $p_t(x)$.</p> <h3 id="continuity-equation">Continuity Equation</h3> <p><strong>In physics, the Continuity Equation is a partial differential equation that describes the transport behavior of conserved quantities</strong>. Under appropriate conditions, mass, energy, momentum, charge, etc., are all conserved quantities, so many transport behaviors can be described using the continuity equation.</p> \[\frac{\partial \rho}{\partial t}+\nabla \cdot (\rho v)=0\] <p>where $\rho$ is the fluid density, $v$ is the fluid velocity vector, $\frac{\partial \rho}{\partial t}$ is the rate of change of fluid density over time, and $\nabla \cdot (\rho v)$ is the divergence of the mass flux density. The meaning of this equation is: the rate of mass change within any closed volume in the fluid equals the difference between the mass flux flowing in and out of that space.</p> <p>By analogy to probability distributions, this equation can be written as:</p> \[\color{red}\frac{\partial p_t(x)}{\partial t}+\nabla \cdot (p_t(x)v_t(x))=0\] <p>In the above equation, $p_t(x)$ is the probability density function at time $t$, and $v_t(x)$ is the vector field associated with $p_t(x)$. This equation is a necessary and sufficient condition for the vector field $v_t(x)$ to generate the probability density path $p_t(x)$, and will be used as a constraint in subsequent derivations.</p> <p>For conditional probability, the above equation becomes</p> \[\frac{\partial p_t(x|x_1)}{\partial t}+\nabla \cdot (p_t(x|x_1)v_t(x|x_1))=0\] <h3 id="conditional-and-marginal-probability-paths-and-vector-fields">Conditional and Marginal Probability Paths and Vector Fields</h3> <p>Since we need to predetermine appropriate $p_t(x)$ and $u_t(x)$ during training, this is obviously difficult. However, we can start from the true distribution $q(x_1)$ and transform the true distribution into other simple distributions through invertible transformations. In this process, we can explicitly write out the <strong>conditional probability path</strong> $p_t(x|x_1)$, and according to the <strong><em>Continuity Equation</em></strong>, we can derive the conditional vector field. <strong><em>By weighting the conditional vector field and conditional probability path (density) using Bayes’ formula, we can recover the marginal probability path and marginal vector field, namely</em></strong> $p_t(x)$ <strong><em>and</em></strong> $u_t(x)$.</p> <p>Since the marginal density is the integral of the conditional density:</p> \[p_t(x)=\int p_t(x|x_1)q(x_1)dx_1\] <p>Thus, its partial derivative with respect to $t$ is:</p> \[\frac{\partial p_t(x)}{\partial t}=\int \frac{\partial p_t(x|x_1)}{\partial t}q(x_1)dx_1=-\int \nabla \cdot (p_t(x|x_1)u_t(x|x_1))q(x_1)dx_1\] <p>According to the continuity equation, if we want a vector field $u_t(x)$ to correspond to the probability path $p_t(x)$, then we must have:</p> \[-\nabla \cdot (p_t(x)u_t(x))=-\int \nabla \cdot (p_t(x|x_1)u_t(x|x_1))q(x_1)dx_1\] <p>We can solve to get</p> \[\color{red}u_t(x)=\int u_t(x|x_1)\frac{p_t(x|x_1)}{p_t(x)}q(x_1)dx_1\] <p>This means that if the marginal vector field $u_t(x)$ is given by the above equation, then it will correspondingly generate the marginal probability path $p_t(x)$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-5-Note-FM/figure4-480.webp 480w,/assets/img/posts/2025-11-5-Note-FM/figure4-800.webp 800w,/assets/img/posts/2025-11-5-Note-FM/figure4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-5-Note-FM/figure4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As shown in the figure above, computing the marginal vector field essentially performs a weighted average over all possible conditional vectors.</p> <h3 id="conditional-flow-matching">Conditional Flow Matching</h3> <p>We find that even though we have the formula for computing $u_t(x)$, the above marginal integral is still intractable, so directly optimizing the <strong><em>Flow Matching</em></strong> objective function is infeasible.</p> <p>The paper proposes the <strong>Conditional Flow Matching</strong> method. <strong>As long as</strong> $u_t(x|x_1)$ <strong>and</strong> $u_t(x)$ <strong>satisfy the above weighted marginal integral condition</strong>, the <strong>Conditional Flow Matching</strong> optimization objective has the same optimal solution as the original <strong>Flow Matching</strong> objective function. The <strong>Conditional Flow Matching</strong> optimization objective is:</p> \[\color{red} \mathcal L_{CFM}(\theta)=\mathbb E_{t\sim U[0,1],x_1\sim q(x_1),x\sim p_t(x|x_1)}\|v_t(x)-u_t(x|x_1)\|^2\] <p>The paper proves through <strong>Theorem1</strong> that the <strong>Conditional Flow Matching</strong> optimization objective and the original <strong>Flow Matching</strong> objective function have the <strong>same gradient</strong>, which means they have the <strong>same optimal solution</strong>.</p> <blockquote> <p><strong><em>Theorem1</em></strong> Assume that for all $x\in\mathbb R^d$ and $t\in [0,1]$, we have $p_t(x)&gt;0$. Then $\mathcal L_{CFM}$ and $\mathcal L_{FM}$ differ by a constant independent of $\theta$, i.e.,</p> \[\nabla _{\theta}\mathcal L_{FM}(\theta)=\nabla _{\theta}\mathcal L_{CFM}(\theta)\] <p><strong><em>Proof</em></strong></p> <p>To ensure the existence of integrals and to facilitate exchanging the order of integration (using Fubini’s theorem), we assume that $q(x)$ and $p_t(x_1)$ decay to 0 sufficiently fast as $|x|\to\infty$, and that $u_t$, $v_t$, $\nabla v_t$ are all bounded.</p> <p>First, we expand the squared norm:</p> \[\begin{align*} \|v_t(x) - u_t(x)\|^2 &amp;= \|v_t(x)\|^2 - 2 \langle v_t(x), u_t(x) \rangle + \|u_t(x)\|^2 \\ \|v_t(x) - u_t(x \mid x_1)\|^2 &amp;= \|v_t(x)\|^2 - 2 \langle v_t(x), u_t(x \mid x_1) \rangle + \|u_t(x \mid x_1)\|^2 \end{align*}\] <p>Next, note that $u_t$ is independent of $\theta$, and we have:</p> \[\begin{align*} \mathbb{E}_{p_t(x)} \|v_t(x)\|^2 &amp;= \int \|v_t(x)\|^2 p_t(x) dx \\ &amp;= \iint \|v_t(x)\|^2 p_t(x \mid x_1) q(x_1) dx_1 dx \\ &amp;= \mathbb{E}_{q(x_1), p_t(x \mid x_1)} \|v_t(x)\|^2 \end{align*}\] <p>Next, we compute:</p> \[\begin{align*} \mathbb{E}_{p_t(x)} \langle v_t(x), u_t(x) \rangle &amp;= \int \left\langle v_t(x), \int u_t(x \mid x_1) \frac{p_t(x \mid x_1) q(x_1)}{p_t(x)} dx_1 \right\rangle p_t(x) dx \\ &amp;= \int \left\langle v_t(x), \int u_t(x \mid x_1) p_t(x \mid x_1) q(x_1) dx_1 \right\rangle dx \\ &amp;= \iint \langle v_t(x), u_t(x \mid x_1) \rangle p_t(x \mid x_1) q(x_1) dx dx_1 \\ &amp;= \mathbb{E}_{q(x_1), p_t(x \mid x_1)} \langle v_t(x), u_t(x \mid x_1) \rangle \end{align*}\] <p>Meanwhile, we note that the third term in the squared norm expansion is a constant independent of $\theta$, thus proving the theorem.</p> </blockquote> <p>Based on the above discussion, the core of constructing a trainable flow model becomes how to design appropriate conditional probability paths and conditional vector fields. Generally speaking, conditional probability paths are easier to construct, while conditional vector fields are more challenging. According to the <strong><em>Continuity Equation</em></strong>, we know</p> \[\frac{\partial p_t(x|x_1)}{\partial t}+\nabla \cdot (p_t(x|x_1)v_t(x|x_1))=0\] <p>This equation allows us to explicitly compute a closed-form $v_t(x|x_1)$ when $p_t(x|x_1)$ is a Gaussian family or a simple function.</p> <h3 id="calculate-conditional-probability-paths-and-conditional-vector-fields">Calculate Conditional Probability Paths and Conditional Vector Fields</h3> <p><strong>Conditional Flow Matching</strong> can choose arbitrary conditional probability paths as long as they satisfy boundary conditions. Here, for simplicity (to obtain a closed-form conditional vector field), we analyze how to construct $p_t(x|x_1)$ and $u_t(x|x_1)$ for general <strong>Gaussian conditional probability paths</strong>.</p> <p>Assume the conditional probability path is a <strong>Gaussian probability path</strong>:</p> \[p_t(x|x_1)=\mathcal N(x|\mu_t(x_1),\sigma_T(x_1)^2I)\] <p>We impose the following constraints on this conditional probability path:</p> <p>At the beginning of time ($t=0$), we satisfy $\mu_0(x_1) = 0, \sigma_0(x_1) = 1$, ensuring that all conditional probability paths converge to the same standard Gaussian noise distribution, i.e., $p(x) = \mathcal{N}(x \mid 0, I)$.</p> <p>At the end of time ($t=1$), we satisfy $\mu_1(x_1) = x_1, \sigma_1(x_1) = \sigma_{\min}$, where $\sigma_{\min}$ should be set sufficiently small to ensure that $p(x \mid x_1)$ is a Gaussian distribution with mean $x_1$ and small variance, so that the conditional probability path converges near $x_1$.</p> <p>This setting defines a deterministic transformation process, starting from a standard Gaussian distribution at $t=0$ and gradually transforming to the target distribution at $t=1$.</p> <p>For a probability path, <strong>there exist infinitely many vector fields that can generate it</strong>, for example, by adding a divergence-free component to the <strong>Continuity Equation</strong>, but this leads to unnecessary computational cost. Here we can use the simplest vector field, <strong>which corresponds to the standard transformation of a Gaussian distribution, with the corresponding data point Flow Map being</strong>:</p> \[x_t=\psi_t(x)=\sigma_t(x_1)x+\mu_t(x_1)\] <p>where $x\sim\mathcal N(0,1)$. $\psi_t(x)$ is an affine transformation that maps $x$ to a normal random variable with mean $\mu_t(x_1)$ and standard deviation $\sigma_t(x_1)$. That is, according to Equation 4, $\psi_t$ pushes the noise distribution $p_0(x|x_1) = p(x)$ to $p_t(x|x_1)$, i.e.,</p> \[\begin{equation} [\psi_t]_∗ p(x) = p_t(x|x_1). \end{equation}\] <p>The flow further defines a vector field that generates the conditional probability path:</p> \[\begin{equation} \frac{d}{dt} \psi_t(x) = u_t(\psi_t(x) \mid x_1). \end{equation}\] <p>Reparametrizing $p_t(x|x_1)$ in terms of $x_0$ and substituting into Equation (13), we obtain the CFM loss function as follows:</p> \[\begin{equation} \mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t, q(x_1), p(x_0)} \left\| v_t(\psi_t(x_0)) - \frac{d}{dt} \psi_t(x_0) \right\|^2. \end{equation}\] <p>Since $\psi_t$ is a simple (invertible) affine mapping, we can utilize Equation (13) to obtain a closed-form solution for $u_t$. Let $f’$ denote the derivative with respect to time $t$, i.e., $f’ = \frac{d}{dt} f$, where $f$ is a time-dependent function.</p> <blockquote> <p><strong><em>Theorem2</em></strong> Let $p_t(x|x_1)$ be the Gaussian probability path as defined in Equation 10, and $\psi_t$ be its corresponding flow map as described in Equation 11. Then the vector field that uniquely defines $\psi_t$ has the following form:</p> \[u_t(x|x_1) = \frac{\sigma'_t(x_1)}{\sigma_t(x_1)} \left( x - \mu_t(x_1) \right) + \mu'_t(x_1)\] <p>Therefore, $u_t(x|x_1)$ generates the Gaussian path $p_t(x|x_1)$</p> <p><strong><em>Proof:</em></strong></p> <p>For notational brevity, let $w_t(x) = u_t(x|x_1)$. Now consider Equation 1:</p> \[\frac{d}{dt}\psi_t(x) = w_t(\psi_t(x)).\] <p>Since $\psi_t$ is invertible (because $\sigma_t(x_1) &gt; 0$), we set $x = \psi_t^{-1}(y)$ and obtain</p> \[\psi_t'(\psi_t^{-1}(y)) = w_t(y)\] <p>Here we use prime notation to denote derivatives, emphasizing that $\psi_t’$ is evaluated at $\psi_t^{-1}(y)$.</p> <p>Now, inverting $\psi_t(x)$ yields</p> \[\psi_t^{-1}(y) = \frac{y - \mu_t(x_1)}{\sigma_t(x_1)}.\] <p>Taking the derivative of $\psi_t$ with respect to $t$ yields</p> \[\psi_t'(x) = \sigma_t'(x_1)x + \mu_t'(x_1).\] <p>Substituting these last two equations into the second line, we obtain</p> \[w_t(y) = \frac{\sigma_t'(x_1)}{\sigma_t(x_1)}(y - \mu_t(x_1)) + \mu_t'(x_1).\] </blockquote> <h2 id="special-instances-of-gaussian-conditional-probability-paths">Special Instances of Gaussian Conditional Probability Paths</h2> <h3 id="diffusion-conditional-vfs">Diffusion conditional VFs</h3> <p><strong>Variance Exploding (VE) and Variance Preserving (VP)</strong> in diffusion models are two different types of diffusion processes used in generative models to simulate two different data distribution change processes.</p> <h4 id="variance-exploding-ve">Variance Exploding (VE)</h4> <p>The VE diffusion model is a diffusion process that increases data variance during the generation process. In this model, as time progresses, data samples gradually become more noisy, with variance continuously increasing until reaching a stable state. A characteristic of the VE process is that it allows the model to explore a wider latent space when generating data, which helps generate diverse samples.</p> <p>The conditional probability path for VE is:</p> \[p_t(x | x_1) = \mathcal{N}(x | x_1, \sigma_{1-t}^2 I)\] <p>where $\sigma_t$ is an increasing function, $\sigma_0 = 0, \sigma_1 \gg 1$, corresponding to mean and standard deviation</p> \[\mu_t(x_1) = x_1, \quad \sigma_t(x_1) = \sigma_{1-t}\] <p>According to Theorem 3, the conditional vector field can be computed as:</p> \[u_t(x | x_1) = - \frac{\sigma'_{1-t}}{\sigma_{1-t}}(x - x_1)\] <h4 id="variance-preserving-vp">Variance Preserving (VP)</h4> <p>The VP diffusion model is a diffusion process that keeps data variance constant during the generation process. In this model, the variance of data samples remains constant throughout the generation process, meaning that while the model introduces noise, it also reduces noise in some way to maintain the overall variance of the data. VP models are typically used in application scenarios that require maintaining data distribution stability, such as maintaining image clarity and structural features in image generation.</p> <p>The conditional probability path for VP is:</p> \[p_t(x | x_1) = \mathcal{N}\left(x \mid \alpha_{1-t}x_1, (1 - \alpha_{1-t}^2)I\right) \text{, where } \alpha_t = e^{-\frac{1}{2}T(t)}, T(t) = \int_0^t \beta(s)ds\] <p>where $\alpha, \beta$ are noise schedule functions, corresponding to mean and standard deviation</p> \[\mu_t(x_1) = \alpha_{1-t}x_1, \quad \sigma_t(x_1) = \sqrt{1 - \alpha_{1-t}^2} \,。\] <p>According to Theorem 3, the conditional vector field can be computed as:</p> \[\begin{align*} u_t(x | x_1) &amp;= \frac{\alpha'_{1-t}}{1-\alpha_{1-t}^2}(\alpha_{1-t}x - x_1) \\ &amp;= -\frac{T'(1-t)}{2}\left[ \frac{e^{-T(1-t)}x - e^{-\frac{1}{2}T(1-t)}x_1}{1-e^{-T(1-t)}} \right] \end{align*}\] <h3 id="optimal-transport-conditional-vfs">Optimal Transport conditional VFs</h3> <p>Optimal Transport (OT) chooses to define the mean and standard deviation of the conditional probability path as simple linear functions. As time $t: 0 \to 1$, corresponding to the probability density path from $p(x) = \mathcal{N}(x | 0, I)$ to $p_1(x | x_1)$, the mean and standard deviation are defined as:</p> \[\mu_t(x_1) = tx_1, \quad \text{and} \quad \sigma_t(x_1) = 1 - (1 - \sigma_{\min})t\] <p>Then the corresponding Flow Map is:</p> \[\psi_t(x) = (1 - (1 - \sigma_{\min})t)x + tx_1\] <p>According to Theorem 3, the closed-form solution of the conditional vector field can be computed as:</p> \[u_t(x | x_1) = \frac{x_1 - (1 - \sigma_{\min})x}{1 - (1 - \sigma_{\min})t}\] <p><strong><em>Optimal transport paths are straight lines, whereas diffusion paths are curves, thus achieving faster training and generation speeds, as well as better performance.</em></strong></p>]]></content><author><name>Runze Tian</name></author><category term="Notes"/><category term="Gen-Model"/><category term="flow-matching"/><category term="ImageGen"/><summary type="html"><![CDATA[This post explores Flow-based Models, Continuous Normalizing Flows (CNFs), and Flow Matching (FM). We discuss Normalizing Flows, derive the conditional flow matching objective, and examine special instances including diffusion models and optimal transport.]]></summary></entry><entry><title type="html">The Unification of DDPM and Score-based Models</title><link href="https://gua927.github.io/blog/2025/Note-Diffusion-DDPM&NCSN/" rel="alternate" type="text/html" title="The Unification of DDPM and Score-based Models"/><published>2025-11-03T14:17:00+00:00</published><updated>2025-11-03T14:17:00+00:00</updated><id>https://gua927.github.io/blog/2025/Note-Diffusion-DDPM&amp;NCSN</id><content type="html" xml:base="https://gua927.github.io/blog/2025/Note-Diffusion-DDPM&amp;NCSN/"><![CDATA[<h2 id="ddpm-from-a-score-perspective">DDPM from a Score Perspective</h2> <p>In <strong><em>DDPM</em></strong>, we know that</p> \[x_t\sim q(x_t|x_0) = N(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I) \tag{1}\] <p>According to <strong><em>Tweedie’s Formula</em></strong>, we can obtain:</p> \[\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0 = \boldsymbol{x}_t + (1 - \bar{\alpha}_t) \nabla_{x_t} \log p(\boldsymbol{x}_t) \tag{2}\] <blockquote> <p><strong><em>Tweedie’s Formula:</em></strong></p> <p>For a Gaussian variable $z\sim \mathcal N(z;\mu_z,\Sigma_z)$, we have</p> \[\mu_z=z+\Sigma_z\nabla_z\log p(z)\] </blockquote> <p>Meanwhile, from (1) we know</p> \[x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon_t\] <p>Substituting into (2), we obtain</p> \[\nabla_{x_t}\log p(x_t)=-\frac{\epsilon_t}{\sqrt{1-\bar{\alpha}_t}}\] <p>Thus</p> \[\begin{align*} \boldsymbol{\mu}_q &amp;= \frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}} \boldsymbol{\varepsilon}_t\\ &amp;=\frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}} \color{red}\nabla_{x_t} \log p(\boldsymbol{x}_t) \end{align*}\] <p>Similarly, we model the reverse process as</p> \[\begin{align*} \boldsymbol{\mu}_{\theta} &amp;= \frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}} \boldsymbol{\varepsilon}_{\theta}(x_t,t)\\ &amp;=\frac{1}{\sqrt{\alpha_t}} \boldsymbol{x}_t + \frac{1 - \alpha_t}{\sqrt{\alpha_t}} \color{red}s_{\theta}(x_t,t) \end{align*}\] <p>Therefore, we transform the estimation of $\epsilon_t$ and $\epsilon_{\theta}$ in <strong><em>DDPM</em></strong> into the estimation of $\nabla\log p(x)$, which gives</p> \[\begin{align*} &amp;\arg\min_{\theta} D_{\text{KL}} \left( q(\boldsymbol{x}_{t-1} \vert \boldsymbol{x}_t, \boldsymbol{x}_0) \parallel p_{\theta}(\boldsymbol{x}_{t-1} \vert \boldsymbol{x}_t) \right) \\ =&amp; \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \lVert \boldsymbol{\mu}_{\theta} - \boldsymbol{\mu}_q \rVert_2^2 \right] \\ =&amp; \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\lVert \frac{1 - \alpha_t}{\sqrt{\alpha_t}} s_{\theta}(\boldsymbol{x}_t, t) - \frac{1 - \alpha_t}{\sqrt{\alpha_t}} \nabla \log p(\boldsymbol{x}_t) \right\rVert_2^2 \right] \\ =&amp; \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \frac{(1 - \alpha_t)^2}{\alpha_t} \left[ \color{red}\lVert s_{\theta}(\boldsymbol{x}_t, t) - \nabla \log p(\boldsymbol{x}_t) \rVert_2^2 \color{black}\right] \end{align*}\] <p>Hence, we find that the optimization objective of DDPM is actually consistent with <strong><em>Score-Based Models</em></strong>, both estimating the <strong><em>score function</em></strong>.</p> <p>We further compare the optimization objective of DDPM:</p> \[L_t = \mathbb{E} \left[ \frac{(1 - \alpha_t)^2}{2\alpha_t (1 - \bar{\alpha}_t) \sigma^2} \color{red}\left\| \boldsymbol{\epsilon}_t - \boldsymbol{\epsilon}_\theta \left( x_t, t \right) \right\|_2^2 \color{black}\right]\] <p>We note that when $L_t$ reaches its optimum, we have</p> \[\epsilon_{\theta}(x_t,t)=\mathbb E\big[\epsilon_t|x_t,t\big]=\mathbb E\big[\epsilon_t|x_0,t\big]\] <p>This indicates that the model actually learns the mean of the noise given data $x_0$. In other words, the conditional expectation learned by our network already contains information about the true sample $x_0$, which is what enables us to obtain the distribution of true data by learning the noise.</p> <p>Meanwhile, we find that under optimal conditions, $L_t$ cannot reach 0, meaning that our optimization objective does not achieve maximum likelihood between the predicted distribution and the true distribution (otherwise the loss should reduce to 0). Combined with <strong><em>Score-Based Models</em></strong>, we know this is because we are actually predicting the <strong><em>score</em></strong> of the data distribution, and there is still a certain gap between the score distribution and the data distribution.</p> <hr/> <h2 id="sde-model">SDE Model</h2> <p>What happens if we extend the finite steps $T$ to infinite steps? Experimental validation shows that larger $T$ can yield more accurate likelihood estimates and better quality results. Thus, continuous-time perturbation of data can be modeled as a stochastic differential equation (SDE).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure1-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure1-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="definition">Definition</h3> <p>There are many forms of SDEs. One form given by Dr. Yang Song in his paper (which can be considered a Diffusion-Type SDE, requiring coefficients to depend only on time <code class="language-plaintext highlighter-rouge">t</code> and current value <code class="language-plaintext highlighter-rouge">x</code>) is:</p> \[\mathrm{d}\boldsymbol{x} = f(\boldsymbol{x}, t)\mathrm{d}t + g(t)\mathrm{d}\boldsymbol{w}\] <p>where $f(\cdot)$ is called the drift coefficient, $g(t)$ is called the diffusion coefficient, $\boldsymbol{w}$ is a standard Brownian motion, and $\mathrm{d}\boldsymbol{w}$ can be viewed as white noise. The solution to this stochastic differential equation is a set of continuous random variables $\lbrace\boldsymbol x(t)\rbrace_{t\in [0,T]}$, where $t$ represents the continuous version of the discrete form $(1, 2, \ldots, T)$.</p> <p>We use $p_t(\boldsymbol{x})$ to denote the probability density function of $\boldsymbol{x}(t)$, which corresponds to the previous $p_{\sigma_t}(\boldsymbol{x}_t)$. Here $p_0(\boldsymbol{x}) = p(\boldsymbol{x})$ is the original data distribution, and $p_T(\boldsymbol{x}) = \mathcal{N}(0, \mathbf{I})$ is the white noise obtained after noise perturbation.</p> <blockquote> <p><strong><em>Brownian Motion</em></strong></p> <p>If a stochastic process $\lbrace X(t), t \geq 0\rbrace$ satisfies:</p> <ul> <li>$X(t)$ is an independent increment process;</li> <li>$\forall s, t &gt; 0, X(s + t) - X(s) \sim N(0, c^2 t)$</li> </ul> <p>then the stochastic process $\lbrace X(t), t \geq 0\rbrace$ is called <strong>Brownian motion</strong> (denoted as $B(t)$) or <strong>Wiener process</strong> (denoted as $W(t)$). In this text, we will subsequently denote it as $W(t)$. If $c = 1$, it is called <strong>standard Brownian motion</strong>, satisfying $W(t) \sim N(0, t)$.</p> </blockquote> <h3 id="forward-sde">Forward SDE</h3> <p>We can directly discretize the equation</p> \[\mathrm{d}\boldsymbol{x} = f(\boldsymbol{x}, t)\mathrm{d}t + g(t)\mathrm{d}\boldsymbol{w}\] <p>where</p> \[dx\to x_{t+\Delta t}-x_t\\ dt\to \Delta t\\ dw\to w(t+\Delta t)-w(t)\sim\mathcal N(0,\Delta t)=\sqrt{\Delta t}\epsilon\] <p>Thus, the discrete form of the SDE is represented as:</p> \[\color{red} x_{t+\Delta t}-x_t=f(x,t)\Delta t+g(t)\sqrt{\Delta t}\epsilon\] <p>where $\epsilon\sim\mathcal N(0,1)$.</p> <h4 id="ve-sde">VE-SDE</h4> <p>For NCSN, the forward process (adding noise) is shown as follows:</p> \[x_t=x_0+\sigma_t\epsilon\\ x_{t+\Delta t}=x_t+\underbrace{\sqrt{\sigma_{t+\Delta t}^2-\sigma_t^2}}_{\color{red}\sqrt{\frac{\sigma_{t+\Delta t}^2-\sigma_t^2}{\Delta t}}\sqrt{\Delta t}}\epsilon\] <p>Therefore, in the corresponding SDE representation</p> \[f(x_t,t)=0\\ g(t)=\lim\limits_{\Delta\to 0}\sqrt{\frac{\sigma_{t+\Delta t}^2-\sigma_t^2}{\Delta t}}=\sqrt{2\sigma_t\dot\sigma_t}\] <p>The corresponding continuous <strong>SDE</strong> for the <strong>VE</strong> process is:</p> \[\color{blue}dx=\sqrt{2\sigma_t\dot\sigma_t}dW\] <h4 id="vp-sde">VP-SDE</h4> <p>For DDPM, its forward process is represented as follows:</p> \[x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon\\ x_{t+1}=\sqrt{1-\beta_{t+1}}x_t+\sqrt{\beta_{t+1}}\epsilon\] <p>We continualize the discrete time $1,2,\cdots,t,\cdots T$ to $[0,1]$, i.e., let</p> \[t\to\frac{t}{T}=t'\\ 1\to\frac{1}{T}=\Delta t\] <p>We can also let $\beta_{t’}=T\beta_t$, thus</p> \[\begin{align*} x_{t'+\Delta t}&amp;=x_{t+1}=\sqrt{1-\beta_{t+1}}x_t+\sqrt{\beta_{t+1}}\epsilon\\ &amp;=\sqrt{1-\beta_{t'+\Delta t}\Delta t}\cdot x_{t'}+\sqrt{\beta_{t'+\Delta t}\Delta t}\cdot \epsilon\\ &amp;=\big(1-\frac{1}{2}\beta_{t'}\Delta t\big)x_{t'}+\sqrt{\beta_{t'}}\sqrt{\Delta t}\epsilon \end{align*}\] <p>Therefore, in the corresponding SDE representation, we have</p> \[f(x_t,t)=-\frac{1}{2}\beta_{t'}x_t\\ g(t)=\sqrt{\beta_{t'}}\] <p>The corresponding continuous <strong>SDE</strong> for the <strong>VP</strong> process is:</p> \[\color{blue}dx=-\frac{1}{2}\beta_{t'}x_tdt+\sqrt{\beta_{t'}}dW\] <p>We expect that when $t\to T$, the image becomes pure noise, then $\sigma_t\to\infty$, but $\bar{\alpha}_t\to 0$ is sufficient. This requires that in <strong>NCSN</strong>, the variance of noise gradually expands, while in <strong>DDPM</strong>, the noise variance remains between $(0,1)$. Therefore, they are respectively called <strong>VE-SDE</strong> and <strong>VP-SDE</strong>.</p> <h3 id="reverse-sde">Reverse SDE</h3> <p>Using the discrete forward <strong>SDE</strong>, we can derive its reverse process. From the forward <strong>SDE</strong>:</p> \[\color{red} x_{t+\Delta t}-x_t=f(x,t)\Delta t+g(t)\sqrt{\Delta t}\epsilon\] <p>we have the conditional probability:</p> \[x_{t+\Delta t}|x_t\sim \mathcal{N}(x_t+f(x_t,t)\Delta t, g^2(t)\Delta tI)\] <p>Considering the reverse process from $x_{t+\Delta t}$ to $x_t$, we have the conditional probability:</p> \[\begin{align*} p(x_t|x_{t+\Delta t})&amp;=\frac{p(x_{t+\Delta t}|x_t)p(x_t)}{p(x_{t+\Delta t})}\\ &amp;=p(x_{t+\Delta t}|x_t)\exp(\log p(x_t)-\log p(x_{t+\Delta t}))\\ &amp;\approx p(x_{t+\Delta t}|x_t)\exp\{\color{red} -(x_{t+\Delta t}-x_t)\nabla_{x_t}\log p(x_t)-\Delta t\frac{\partial}{\partial t}\log p(x_t) \color{black}\}\\ &amp;\propto \exp\{ -\frac{\|x_{t+\Delta t}-x_t-f(x_t,t)\Delta t\|_2^2}{2g^2(t)\Delta t} - (x_{t+\Delta t}-x_t)\nabla_{x_t}\log p(x_t)-\Delta t\frac{\partial}{\partial t}\log p(x_t) \}\\ &amp;=\exp\bigg\{ -\frac{1}{2g^2(t)\Delta t}\|(x_{t+\Delta t}-x_t)-\big[f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \big]\Delta t\|_2^2 - \Delta t\frac{\partial}{\partial t}\log p(x_t)-\frac{f^2(x_t,t)\Delta t}{2g^2(t)}+\frac{\|f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \|_2^2\Delta t}{2g^2(t)} \bigg\}\\ &amp;\stackrel{\Delta t \to 0}{=} \exp\bigg\{ -\frac{1}{2g^2(t)\Delta t}\| (x_{t+\Delta t}-x_t)-\big[f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \big]\Delta t \|_2^2 \bigg\} \end{align*}\] <p>Therefore, $x_t|x_{t+\Delta t}$ follows a Gaussian distribution with mean and variance as follows:</p> \[\mu=x_{t+\Delta t}-\big[f(x_t,t)-g^2(t)\nabla_{x_t}\log p(x_t) \big]\Delta t\\ \sigma^2=g^2(t)\Delta t\] <p>Thus, we can obtain both the discrete and continuous forms of the reverse SDE process:</p> \[x_{t+\Delta t}-x_t=\big[f(x_t+\Delta t,t+\Delta t)-g^2(t+\Delta t)\nabla_{x_t+\Delta t}\log p(x_{t+\Delta t}) \big]\Delta t+g(t+\Delta t)\sqrt{\Delta t}\epsilon\] \[\color{blue}dx=\big[f(x_t,t)-g^2(t)\color{red}\nabla_{x_t}\log p(x_t)\color{blue} \big]dt+g(t)dW\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure2-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure2-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Therefore, after we have learned the <strong><em>score function</em></strong>, the reverse process becomes completely solvable. During generation, we start by sampling $x_T \sim \mathcal{N}(0, 1)$, and gradually obtain $x_0$ using the discrete process above. This discretization method for stochastic differential equations is also called the <strong>Euler–Maruyama method</strong>.</p> <p>For NCSN, its forward VE process continuous SDE is:</p> \[\color{blue}dx=\sqrt{2\sigma_t\dot\sigma_t}dW\] <p>Then its reverse process is:</p> \[dx=-2\sigma_t\dot\sigma_t\nabla_{x_t}\log p(x_t)dt+\sqrt{2\sigma_t\dot\sigma_t}dW\] <p>Written in discrete form, it becomes</p> \[x_t-x_{t-1}=-2\sigma_t\dot\sigma_t\nabla_{x_t}\log p(x_t)\Delta t+\sqrt{2\sigma_t\dot\sigma_t}\sqrt{\Delta t}\epsilon_t\] <p>In the case where $\sigma_t\dot\sigma_t=1$, if we set $\Delta t=\delta$ (this is done only for formal consistency, without real meaning), then we have</p> \[x_{t-1}=x_t+2\delta\nabla_{x_t}\log p(x_t)+\sqrt{2\delta}\epsilon\] <p>This also unifies with the <strong><em>Langevin Equation</em></strong> mentioned earlier.</p> <h3 id="optimization-target">Optimization Target</h3> <p>Solving the reverse SDE requires us to know the terminal distribution $p_T(\boldsymbol{x})$ and the score function $\nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})$. By design, the former is close to the prior distribution $\pi(\boldsymbol{x})$, which is fully tractable.</p> <p>To estimate $\nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})$, we train a <strong>time-dependent score-based model</strong> $s_\theta(\boldsymbol{x}, t)$ such that $s_\theta(\boldsymbol{x}, t) \approx \nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})$. This is similar to NCSN’s $s_\theta(\boldsymbol{x}, i)$, which after training satisfies $s_\theta(\boldsymbol{x}, i) \approx \nabla_{\boldsymbol{x}} \log p_{\sigma_i}(\boldsymbol{x})$.</p> <p>Our training objective for $s_\theta(\boldsymbol{x}, t)$ is a continuous weighted combination of Fisher divergences, given by:</p> \[\mathbb{E}_{t \in \mathcal{U}(0,T)} \mathbb{E}_{p_t(\mathbf{x})} \left[ \lambda(t) \left\| \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, t) \right\|_2^2 \right]\] <h4 id="relationship-with-likelihood">Relationship with Likelihood</h4> <p>When $\lambda(t)=g^2(t)$, under some regularity conditions, there exists an important connection between our weighted combination of Fisher divergences and the KL divergence from $p_0$ to $p_\theta$:</p> \[\begin{align*} \mathrm{KL}(p_0(\mathbf{x}) \| p_\theta(\mathbf{x})) &amp;\leq \frac{T}{2} \mathbb{E}_{t \in \mathcal{U}(0,T)} \mathbb{E}_{p_t(\mathbf{x})} \left[ \lambda(t) \| \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, t) \|_2^2 \right] \\ &amp;+ \mathrm{KL}(p_T \| \pi) \end{align*}\] <p>Due to this special connection with KL divergence, and the equivalence between minimizing KL divergence and maximizing likelihood in model training, we call $\lambda(t)=g(t)^2$ the <strong>likelihood weighting function</strong>. Using this likelihood weighting function, we can train score-based generative models to achieve very high likelihoods.</p> <h3 id="pc-sampling">PC Sampling</h3> <p>Reviewing DDPM and NCSN from an algorithmic implementation perspective, DDPM is based on the Markov assumption, assuming that samples at different times follow conditional probability distributions. Therefore, DDPM uses Ancestral Sampling to solve the SDE equation, with the algorithm shown below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure3-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure3-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>While NCSN relies on Langevin Dynamics for iterative optimization under the same noise distribution. For different noise magnitudes, there is no dependency relationship between the obtained samples. Its sampling method is shown as follows:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure4-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure4-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The former can be seen as solving the discrete form of the SDE equation, called the Predictor, while the latter can be seen as a further optimization process, called the Corrector. The author combines these two parts to present the Predictor-Corrector Sampling Method:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure5-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure5-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="probability-flow-ode">Probability Flow ODE</h2> <p>We can transform any <strong><em>SDE</em></strong> into an <strong><em>ODE</em></strong> without changing the marginal distributions $\lbrace p_t(x)\rbrace_{t\in[0,T]}$ of the stochastic differential equation. Therefore, by solving this <strong><em>ODE</em></strong>, we can sample from the same distribution as the <strong><em>Reverse SDE</em></strong>. The <strong><em>ODE</em></strong> corresponding to the <strong><em>SDE</em></strong> is called the <strong><em>Probability flow ODE</em></strong>, with the form:</p> \[\color{blue}dx=\big[f(x_t,t)-\frac{1}{2}g^2(t)\color{red}\nabla_{x_t}\log p(x_t)\color{blue} \big]dt\] <p>The figure below depicts trajectories of stochastic differential equations (SDEs) and probability flow ordinary differential equations (ODEs). It can be seen that ODE trajectories are significantly smoother than SDE trajectories, and they transform the same data distribution into the same prior distribution and vice versa, sharing the same set of marginal distributions $\lbrace p_t(\boldsymbol{x})\rbrace_{t\in[0,T]}$. In other words, trajectories obtained by solving the probability flow ODE have the same marginal distributions as SDE trajectories.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-11-03-Note-Diffusion/figure6-480.webp 480w,/assets/img/posts/2025-11-03-Note-Diffusion/figure6-800.webp 800w,/assets/img/posts/2025-11-03-Note-Diffusion/figure6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/posts/2025-11-03-Note-Diffusion/figure6.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Using <strong><em>probability flow ODEs</em></strong> provides many benefits:</p> <ol> <li>Exact likelihood computation</li> <li>Manipulating latent representations</li> <li>Uniquely identifiable encoding</li> <li>Efficient sampling</li> </ol> <hr/> <h2 id="conditional-generation">Conditional Generation</h2> <p>According to Bayes’ theorem</p> \[p(x|y)=\frac{p(x)p(y|x)}{p(y)}\] <p>Taking the score with respect to $x$ on both sides gives</p> \[\nabla_x\log p(x|y)=\nabla_x\log p(x)+\nabla_x\log p(y|x)\] <p>Both latter terms are <strong><em>score functions</em></strong> that we can estimate. Therefore, we can generate $p(x|y)$ by solving the reverse SDE.</p>]]></content><author><name>Runze Tian</name></author><category term="Notes"/><category term="Gen-Model"/><category term="diffusion-model"/><category term="score-matching"/><category term="ImageGen"/><summary type="html"><![CDATA[This post explores the unification of DDPM and Score-based Models in diffusion generative modeling. We show how x-prediction and score-prediction are fundamentally equivalent, and how both can be viewed through the lens of Stochastic Differential Equations (SDEs).]]></summary></entry></feed>